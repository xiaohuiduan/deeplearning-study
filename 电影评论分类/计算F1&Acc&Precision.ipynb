{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.nn import utils as nn_utils\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = np.load(\"./output/word2idx.npy\",allow_pickle=True).item()\n",
    "idx2word = {v:k for k,v in word2idx.items()}\n",
    "embedding_weight = torch.load(\"./output/embedding_weight.h5\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_file_data_x_y(word2idx,file_path):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(file_path) as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            data = line.split()\n",
    "            if(len(data) == 0):\n",
    "                continue\n",
    "            x = [word2idx[i] if i in word2idx.keys() else word2idx[\"<unk>\"] for i in data[1:]]\n",
    "            y = int(data[0])\n",
    "            X.append(x) \n",
    "            Y.append(y)\n",
    "    return X,Y\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.len = len(X)\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index],self.Y[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def collate_fn(batch_data):\n",
    "    \"\"\"\n",
    "        batch_data的shape：(batch_size,变长句子,1)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    Y = []\n",
    "    for data in batch_data:\n",
    "        X.append(torch.LongTensor(data[0]))\n",
    "        Y.append(data[1])    \n",
    "    data_len = [len(i) for i in X]\n",
    "\n",
    "    input_data = nn_utils.rnn.pad_sequence(X,batch_first=True,padding_value=0) # 因为<pad>=0，所以padding_value=0\n",
    "    return input_data,torch.LongTensor(Y),data_len\n",
    "test_X,test_Y = return_file_data_x_y(word2idx,\"./data/test_zh.txt\")\n",
    "test_dataset = CommentDataset(test_X,test_Y)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=256,collate_fn=collate_fn,num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算混淆矩阵函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_confusion_matrix(my_net,test_dataloader,is_lstm = False):\n",
    "    real_Y = []\n",
    "    predict_Y = []\n",
    "    for i,data in enumerate(test_dataloader):\n",
    "        inputs = data[0].to(device)\n",
    "        target = data[1].numpy()\n",
    "        data_len = data[2]\n",
    "        if is_lstm:\n",
    "            outputs= my_net(inputs,data_len)\n",
    "        else:\n",
    "            outputs= my_net(inputs)\n",
    "        _,top_index = torch.max(outputs,1)\n",
    "        top_index = top_index.cpu().numpy() # [1,0,0,1]\n",
    "        \n",
    "        predict_Y.extend(top_index)\n",
    "        real_Y.extend(target)\n",
    "    C = confusion_matrix(real_Y,predict_Y)\n",
    "    R = classification_report(real_Y,predict_Y,digits=4)\n",
    "\n",
    "    return C,R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试不同模型结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,embedding_size):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weight,freeze=False)\n",
    "        self.conv = nn.Conv2d(1,256,(3,embedding_size)) # kernel_size 为(3,embedding_size)\n",
    "        self.adaptive_max_pool = nn.AdaptiveMaxPool1d(2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*2,128),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128,2),   \n",
    "        )\n",
    "    \n",
    "    def forward(self,x): # (batch_size,seq_len)\n",
    "        x = self.embedding(x) #(batch_size,seq_len,embedding_size)\n",
    "        x = x.unsqueeze(1) # (batch_size,1,seq_len,embedding_size) ，因为CNN的input为(N,C,H,W)\n",
    "        x = self.conv(x) #(batch_size,256,seq_len-2,1)\n",
    "        x = x.squeeze(3) #(batch_size,256,seq_len-2)\n",
    "        x = F.relu(x)\n",
    "        x = self.adaptive_max_pool(x) #(batch_size,256,2)\n",
    "        x = torch.cat((x[:,:,0],x[:,:,1]),dim=1) #(batch_size,256*2)\n",
    "        output = self.fc(x)\n",
    "        return F.log_softmax(output,dim=1)\n",
    "my_net = torch.load(\"./output/text_cnn.h5\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8196    0.8736    0.8457       182\n",
      "           1     0.8686    0.8128    0.8398       187\n",
      "\n",
      "    accuracy                         0.8428       369\n",
      "   macro avg     0.8441    0.8432    0.8428       369\n",
      "weighted avg     0.8444    0.8428    0.8427       369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C,R = compute_confusion_matrix(my_net,test_dataloader)\n",
    "print(\"TextCNN\\n\")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLstm with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self,embedding_size,hidden_size,num_layers=1):\n",
    "        super(MyNet,self).__init__()\n",
    "        # 使用与训练好的词向量权重\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weight,freeze=False) \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,batch_first=True,bidirectional=True,num_layers=self.num_layers)\n",
    "        # self.w = nn.Parameter(torch.Tensor(hidden_size,1))\n",
    "        self.attention_w = nn.Sequential(\n",
    "            nn.Linear(hidden_size,hidden_size),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size,256),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,2),\n",
    "        )\n",
    "        \n",
    "    def attention_layer(self,lstm_output,lstm_h_n = None):\n",
    "        \"\"\"\n",
    "            lstm_output：(batch_size,seq_len,hidden_size*2)\n",
    "            lstm_h_n：(num_layers*2,batch_size,hidden_size)\n",
    "        \"\"\"\n",
    "        lstm_h_n = lstm_h_n.permute(1,0,2) # (batch_size,num_layers*2,hidden_size)\n",
    "        lstm_h_n = torch.sum(lstm_h_n,dim=1) # (batch_size,hidden_size)\n",
    "        attention_w = self.attention_w(lstm_h_n) # (batch_size,hidden_size)\n",
    "        attention_w = attention_w.unsqueeze(dim=2) # (batch_size,hidden_size,1)\n",
    "        H = lstm_output[:,:,:self.hidden_size] + lstm_output[:,:,self.hidden_size:] # (batch_size,seq_len,hidden_size)\n",
    "        # alpha = F.softmax(torch.matmul(H,self.w),dim=1) #(batch_size,seq_len,1)\n",
    "        alpha = F.softmax(torch.matmul(H,attention_w),dim=1) #(batch_size,seq_len,1)\n",
    "        r = H * alpha # (batch_size,seq_len,hidden_size)\n",
    "        out = torch.relu(torch.sum(r,1)) #(batch_size,hidden_size)\n",
    "        return out\n",
    "    \n",
    "    def forward(self,input,data_len=None):\n",
    "        input = self.embedding(input)\n",
    "        input = nn_utils.rnn.pack_padded_sequence(input,data_len,batch_first=True,enforce_sorted=False)\n",
    "        output,(h_n,c_n) = self.lstm(input) # output (batch_size,seq_len,hidden_size*2) h_n(num_layers*2,batch_size,hidden_size)\n",
    "        output,_ = nn_utils.rnn.pad_packed_sequence(output,batch_first=True)\n",
    "        output = self.attention_layer(output,h_n) #(batch_size,hidden_size)\n",
    "        output = self.fc(output) # (batch_size,2)\n",
    "        return F.log_softmax(output,dim=1)\n",
    "    \n",
    "my_net = torch.load(\"./output/bi_lstm.h5\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM with Attention\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8564    0.8516    0.8540       182\n",
      "           1     0.8564    0.8610    0.8587       187\n",
      "\n",
      "    accuracy                         0.8564       369\n",
      "   macro avg     0.8564    0.8563    0.8563       369\n",
      "weighted avg     0.8564    0.8564    0.8564       369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C,R = compute_confusion_matrix(my_net,test_dataloader,True)\n",
    "print(\"BiLSTM with Attention\\n\")\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,embedding_size,hidden_size,num_layers=2):\n",
    "        super(MyNet,self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_weight,freeze=False) \n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_size,hidden_size,batch_first=True,bidirectional=True,num_layers=self.num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Linear(hidden_size,128),\n",
    "            nn.Dropout(0.6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,2),   \n",
    "        )\n",
    "    \n",
    "    def forward(self,input,data_len):\n",
    "        \n",
    "        input = self.embedding(input)\n",
    "        input = nn_utils.rnn.pack_padded_sequence(input,data_len,batch_first=True,enforce_sorted=False)\n",
    "        _,(h_n,c_n) = self.lstm(input) # h_n(num_layers*2,batch_size,hidden_size)\n",
    "        h_n = torch.permute(h_n,(1,0,2)) # h_n(batch_size,num_layers*2,hidden_size)\n",
    "        h_n = torch.sum(h_n,dim=1) # h_n (batch_size,hidden_size)\n",
    "        output = self.fc(h_n)\n",
    "        return F.log_softmax(output,dim=1)\n",
    "        \n",
    "my_net = torch.load(\"./output/lstm.h5\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7740    0.8846    0.8256       182\n",
      "           1     0.8696    0.7487    0.8046       187\n",
      "\n",
      "    accuracy                         0.8157       369\n",
      "   macro avg     0.8218    0.8166    0.8151       369\n",
      "weighted avg     0.8224    0.8157    0.8150       369\n",
      "\n"
     ]
    }
   ],
   "source": [
    "C,R = compute_confusion_matrix(my_net,test_dataloader,True)\n",
    "print(\"LSTM\\n\")\n",
    "print(R)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1f19b4cb6f07787e8aa8a4ed8d21d7c54c8571ee1dd17ba5dfeacd1c5a2f4df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
